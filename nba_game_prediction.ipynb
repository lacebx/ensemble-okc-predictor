{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Game Prediction: OKC Thunder Next 5 Games\n",
    "\n",
    "## Project Overview\n",
    "This project uses ensemble learning techniques to predict the next 5 game results for the Oklahoma City Thunder (OKC).\n",
    "\n",
    "**Current Status**: OKC Thunder is on a 22-1 streak (22 wins, 1 loss)\n",
    "\n",
    "## Dataset\n",
    "- Source: NBA team statistics dataset\n",
    "- Target Variable (y): Game outcome (Win/Loss)\n",
    "- Input Features (X): Team statistics, opponent statistics, and derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "team_stats = pd.read_csv('dataset/Team Stats Per Game.csv')\n",
    "team_summaries = pd.read_csv('dataset/Team Summaries.csv')\n",
    "opponent_stats = pd.read_csv('dataset/Opponent Stats Per Game.csv')\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n",
    "print(f\"\\nTeam Stats shape: {team_stats.shape}\")\n",
    "print(f\"Team Summaries shape: {team_summaries.shape}\")\n",
    "print(f\"Opponent Stats shape: {opponent_stats.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current OKC Thunder stats (2026 season)\n",
    "okc_2026 = team_summaries[(team_summaries['abbreviation'] == 'OKC') & (team_summaries['season'] == 2026)]\n",
    "print(\"OKC Thunder 2026 Season Stats:\")\n",
    "print(okc_2026[['season', 'w', 'l', 'mov', 'srs', 'o_rtg', 'd_rtg', 'n_rtg']].to_string())\n",
    "\n",
    "# Display first few rows of team stats\n",
    "print(\"\\n\\nSample Team Stats:\")\n",
    "print(team_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets to create comprehensive team statistics\n",
    "def create_game_dataset(team_stats, team_summaries, opponent_stats):\n",
    "    \"\"\"\n",
    "    Create a dataset where each row represents a potential game matchup.\n",
    "    Since we don't have actual game-by-game data, we'll create matchups\n",
    "    based on team statistics from the same season.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for recent seasons (last 5 years for better relevance)\n",
    "    recent_seasons = team_stats[team_stats['season'] >= 2021].copy()\n",
    "    \n",
    "    games = []\n",
    "    \n",
    "    for season in recent_seasons['season'].unique():\n",
    "        season_teams = recent_seasons[recent_seasons['season'] == season].copy()\n",
    "        season_summaries = team_summaries[team_summaries['season'] == season].copy()\n",
    "        season_opp = opponent_stats[opponent_stats['season'] == season].copy()\n",
    "        \n",
    "        # Get team abbreviations\n",
    "        teams = season_teams['abbreviation'].unique()\n",
    "        teams = [t for t in teams if t != 'NA']  # Remove league average\n",
    "        \n",
    "        # Create matchups (each team plays every other team)\n",
    "        for i, team1 in enumerate(teams):\n",
    "            for team2 in teams[i+1:]:\n",
    "                team1_data = season_teams[season_teams['abbreviation'] == team1].iloc[0]\n",
    "                team2_data = season_teams[season_teams['abbreviation'] == team2].iloc[0]\n",
    "                \n",
    "                team1_summary = season_summaries[season_summaries['abbreviation'] == team1]\n",
    "                team2_summary = season_summaries[season_summaries['abbreviation'] == team2]\n",
    "                \n",
    "                team1_opp = season_opp[season_opp['abbreviation'] == team1]\n",
    "                team2_opp = season_opp[season_opp['abbreviation'] == team2]\n",
    "                \n",
    "                if len(team1_summary) > 0 and len(team2_summary) > 0:\n",
    "                    # Determine winner based on win percentage\n",
    "                    team1_wpct = team1_summary.iloc[0]['w'] / (team1_summary.iloc[0]['w'] + team1_summary.iloc[0]['l'])\n",
    "                    team2_wpct = team2_summary.iloc[0]['w'] / (team2_summary.iloc[0]['w'] + team2_summary.iloc[0]['l'])\n",
    "                    \n",
    "                    # Create features for team1 perspective\n",
    "                    game_features = {\n",
    "                        'season': season,\n",
    "                        # Team 1 offensive stats\n",
    "                        'team1_pts_per_game': team1_data['pts_per_game'],\n",
    "                        'team1_fg_percent': team1_data['fg_percent'],\n",
    "                        'team1_3p_percent': team1_data['x3p_percent'],\n",
    "                        'team1_ft_percent': team1_data['ft_percent'],\n",
    "                        'team1_reb_per_game': team1_data['trb_per_game'],\n",
    "                        'team1_ast_per_game': team1_data['ast_per_game'],\n",
    "                        'team1_stl_per_game': team1_data['stl_per_game'],\n",
    "                        'team1_blk_per_game': team1_data['blk_per_game'],\n",
    "                        'team1_tov_per_game': team1_data['tov_per_game'],\n",
    "                        \n",
    "                        # Team 1 advanced stats\n",
    "                        'team1_o_rtg': team1_summary.iloc[0]['o_rtg'],\n",
    "                        'team1_d_rtg': team1_summary.iloc[0]['d_rtg'],\n",
    "                        'team1_n_rtg': team1_summary.iloc[0]['n_rtg'],\n",
    "                        'team1_srs': team1_summary.iloc[0]['srs'],\n",
    "                        'team1_mov': team1_summary.iloc[0]['mov'],\n",
    "                        \n",
    "                        # Team 2 (opponent) offensive stats\n",
    "                        'team2_pts_per_game': team2_data['pts_per_game'],\n",
    "                        'team2_fg_percent': team2_data['fg_percent'],\n",
    "                        'team2_3p_percent': team2_data['x3p_percent'],\n",
    "                        'team2_ft_percent': team2_data['ft_percent'],\n",
    "                        'team2_reb_per_game': team2_data['trb_per_game'],\n",
    "                        'team2_ast_per_game': team2_data['ast_per_game'],\n",
    "                        'team2_stl_per_game': team2_data['stl_per_game'],\n",
    "                        'team2_blk_per_game': team2_data['blk_per_game'],\n",
    "                        'team2_tov_per_game': team2_data['tov_per_game'],\n",
    "                        \n",
    "                        # Team 2 advanced stats\n",
    "                        'team2_o_rtg': team2_summary.iloc[0]['o_rtg'],\n",
    "                        'team2_d_rtg': team2_summary.iloc[0]['d_rtg'],\n",
    "                        'team2_n_rtg': team2_summary.iloc[0]['n_rtg'],\n",
    "                        'team2_srs': team2_summary.iloc[0]['srs'],\n",
    "                        'team2_mov': team2_summary.iloc[0]['mov'],\n",
    "                        \n",
    "                        # Derived features (differences)\n",
    "                        'pts_diff': team1_data['pts_per_game'] - team2_data['pts_per_game'],\n",
    "                        'rtg_diff': team1_summary.iloc[0]['n_rtg'] - team2_summary.iloc[0]['n_rtg'],\n",
    "                        'srs_diff': team1_summary.iloc[0]['srs'] - team2_summary.iloc[0]['srs'],\n",
    "                        'off_def_diff': (team1_summary.iloc[0]['o_rtg'] - team2_summary.iloc[0]['d_rtg']),\n",
    "                        \n",
    "                        # Target: 1 if team1 wins, 0 if team2 wins\n",
    "                        'team1_wins': 1 if team1_wpct > team2_wpct else 0\n",
    "                    }\n",
    "                    \n",
    "                    games.append(game_features)\n",
    "    \n",
    "    return pd.DataFrame(games)\n",
    "\n",
    "# Create the game dataset\n",
    "game_data = create_game_dataset(team_stats, team_summaries, opponent_stats)\n",
    "print(f\"\\nGame dataset created: {game_data.shape}\")\n",
    "print(f\"\\nColumns: {list(game_data.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(game_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(game_data.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "game_data = game_data.dropna()\n",
    "\n",
    "# Check for outliers (using IQR method)\n",
    "numeric_cols = game_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols.remove('team1_wins')  # Don't check target variable\n",
    "\n",
    "print(f\"\\nDataset after cleaning: {game_data.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(game_data['team1_wins'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "feature_cols = [col for col in game_data.columns if col not in ['season', 'team1_wins']]\n",
    "X = game_data[feature_cols]\n",
    "y = game_data['team1_wins']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {feature_cols}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementation - Ensemble Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "\n",
    "# Cross-validation score\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
    "print(f\"\\nCross-validation scores: {rf_cv_scores}\")\n",
    "print(f\"Mean CV score: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, gb_pred))\n",
    "\n",
    "# Cross-validation score\n",
    "gb_cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5)\n",
    "print(f\"\\nCross-validation scores: {gb_cv_scores}\")\n",
    "print(f\"Mean CV score: {gb_cv_scores.mean():.4f} (+/- {gb_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Classifier\n",
    "bagging_model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_pred = bagging_model.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
    "\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, bagging_pred))\n",
    "\n",
    "# Cross-validation score\n",
    "bagging_cv_scores = cross_val_score(bagging_model, X_train, y_train, cv=5)\n",
    "print(f\"\\nCross-validation scores: {bagging_cv_scores}\")\n",
    "print(f\"Mean CV score: {bagging_cv_scores.mean():.4f} (+/- {bagging_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier (combines RF, GB, and Bagging)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=5, random_state=42)),\n",
    "        ('bag', BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=10), n_estimators=30, random_state=42, n_jobs=-1))\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "voting_pred = voting_model.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, voting_pred))\n",
    "\n",
    "# Cross-validation score\n",
    "voting_cv_scores = cross_val_score(voting_model, X_train, y_train, cv=5)\n",
    "print(f\"\\nCross-validation scores: {voting_cv_scores}\")\n",
    "print(f\"Mean CV score: {voting_cv_scores.mean():.4f} (+/- {voting_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "models_comparison = {\n",
    "    'Random Forest': {'accuracy': rf_accuracy, 'cv_mean': rf_cv_scores.mean(), 'cv_std': rf_cv_scores.std()},\n",
    "    'Gradient Boosting': {'accuracy': gb_accuracy, 'cv_mean': gb_cv_scores.mean(), 'cv_std': gb_cv_scores.std()},\n",
    "    'Bagging': {'accuracy': bagging_accuracy, 'cv_mean': bagging_cv_scores.mean(), 'cv_std': bagging_cv_scores.std()},\n",
    "    'Voting Classifier': {'accuracy': voting_accuracy, 'cv_mean': voting_cv_scores.mean(), 'cv_std': voting_cv_scores.std()}\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_comparison).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(comparison_df.index, comparison_df['accuracy'], color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])\n",
    "axes[0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cross-validation mean scores\n",
    "axes[1].bar(comparison_df.index, comparison_df['cv_mean'], yerr=comparison_df['cv_std'], \n",
    "            color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], capsize=5)\n",
    "axes[1].set_title('Cross-Validation Mean Scores', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('CV Mean Score', fontsize=12)\n",
    "axes[1].set_ylim([0.5, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df['cv_mean'].idxmax()\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best CV Score: {comparison_df.loc[best_model_name, 'cv_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest (best model typically)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Top 15 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictions for OKC Thunder Next 5 Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OKC Thunder current stats (2026 season)\n",
    "okc_stats = team_stats[(team_stats['abbreviation'] == 'OKC') & (team_stats['season'] == 2026)].iloc[0]\n",
    "okc_summary = team_summaries[(team_summaries['abbreviation'] == 'OKC') & (team_summaries['season'] == 2026)].iloc[0]\n",
    "\n",
    "# Get potential opponents (other teams from 2026 season)\n",
    "opponents_2026 = team_stats[(team_stats['season'] == 2026) & (team_stats['abbreviation'] != 'OKC') & (team_stats['abbreviation'] != 'NA')]\n",
    "\n",
    "# Select 5 opponents (we'll use teams with varying strengths)\n",
    "# Let's pick a mix: strong teams, average teams, and weaker teams\n",
    "opponent_abbrevs = ['DEN', 'LAL', 'HOU', 'NYK', 'SAC']  # Mix of strong and weaker teams\n",
    "\n",
    "print(\"OKC Thunder Current Stats (2026):\")\n",
    "print(f\"Wins: {okc_summary['w']}, Losses: {okc_summary['l']}\")\n",
    "print(f\"Net Rating: {okc_summary['n_rtg']:.2f}\")\n",
    "print(f\"Points Per Game: {okc_stats['pts_per_game']:.1f}\")\n",
    "print(f\"\\nPredicting next 5 games against:\")\n",
    "for opp in opponent_abbrevs:\n",
    "    opp_data = team_summaries[(team_summaries['abbreviation'] == opp) & (team_summaries['season'] == 2026)]\n",
    "    if len(opp_data) > 0:\n",
    "        print(f\"  - {opp}: {opp_data.iloc[0]['w']}-{opp_data.iloc[0]['l']} record\")\n",
    "\n",
    "def create_prediction_features(okc_stats, okc_summary, opponent_stats, opponent_summary):\n",
    "    \"\"\"Create features for a single game prediction\"\"\"\n",
    "    return pd.DataFrame([{\n",
    "        'team1_pts_per_game': okc_stats['pts_per_game'],\n",
    "        'team1_fg_percent': okc_stats['fg_percent'],\n",
    "        'team1_3p_percent': okc_stats['x3p_percent'],\n",
    "        'team1_ft_percent': okc_stats['ft_percent'],\n",
    "        'team1_reb_per_game': okc_stats['trb_per_game'],\n",
    "        'team1_ast_per_game': okc_stats['ast_per_game'],\n",
    "        'team1_stl_per_game': okc_stats['stl_per_game'],\n",
    "        'team1_blk_per_game': okc_stats['blk_per_game'],\n",
    "        'team1_tov_per_game': okc_stats['tov_per_game'],\n",
    "        'team1_o_rtg': okc_summary['o_rtg'],\n",
    "        'team1_d_rtg': okc_summary['d_rtg'],\n",
    "        'team1_n_rtg': okc_summary['n_rtg'],\n",
    "        'team1_srs': okc_summary['srs'],\n",
    "        'team1_mov': okc_summary['mov'],\n",
    "        'team2_pts_per_game': opponent_stats['pts_per_game'],\n",
    "        'team2_fg_percent': opponent_stats['fg_percent'],\n",
    "        'team2_3p_percent': opponent_stats['x3p_percent'],\n",
    "        'team2_ft_percent': opponent_stats['ft_percent'],\n",
    "        'team2_reb_per_game': opponent_stats['trb_per_game'],\n",
    "        'team2_ast_per_game': opponent_stats['ast_per_game'],\n",
    "        'team2_stl_per_game': opponent_stats['stl_per_game'],\n",
    "        'team2_blk_per_game': opponent_stats['blk_per_game'],\n",
    "        'team2_tov_per_game': opponent_stats['tov_per_game'],\n",
    "        'team2_o_rtg': opponent_summary['o_rtg'],\n",
    "        'team2_d_rtg': opponent_summary['d_rtg'],\n",
    "        'team2_n_rtg': opponent_summary['n_rtg'],\n",
    "        'team2_srs': opponent_summary['srs'],\n",
    "        'team2_mov': opponent_summary['mov'],\n",
    "        'pts_diff': okc_stats['pts_per_game'] - opponent_stats['pts_per_game'],\n",
    "        'rtg_diff': okc_summary['n_rtg'] - opponent_summary['n_rtg'],\n",
    "        'srs_diff': okc_summary['srs'] - opponent_summary['srs'],\n",
    "        'off_def_diff': okc_summary['o_rtg'] - opponent_summary['d_rtg'],\n",
    "    }])\n",
    "\n",
    "# Make predictions for each opponent\n",
    "predictions = []\n",
    "for opp_abbrev in opponent_abbrevs:\n",
    "    opp_stats = team_stats[(team_stats['abbreviation'] == opp_abbrev) & (team_stats['season'] == 2026)]\n",
    "    opp_summary = team_summaries[(team_summaries['abbreviation'] == opp_abbrev) & (team_summaries['season'] == 2026)]\n",
    "    \n",
    "    if len(opp_stats) > 0 and len(opp_summary) > 0:\n",
    "        game_features = create_prediction_features(okc_stats, okc_summary, opp_stats.iloc[0], opp_summary.iloc[0])\n",
    "        \n",
    "        # Use the best model (Random Forest) for predictions\n",
    "        win_prob = rf_model.predict_proba(game_features)[0][1]\n",
    "        prediction = rf_model.predict(game_features)[0]\n",
    "        \n",
    "        predictions.append({\n",
    "            'Opponent': opp_abbrev,\n",
    "            'Opponent Record': f\"{opp_summary.iloc[0]['w']}-{opp_summary.iloc[0]['l']}\",\n",
    "            'Prediction': 'WIN' if prediction == 1 else 'LOSS',\n",
    "            'Win Probability': f\"{win_prob*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(\"\\n\\nPredictions for OKC Thunder Next 5 Games:\")\n",
    "print(\"=\" * 60)\n",
    "print(predictions_df.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "wins = sum([1 for p in predictions if p['Prediction'] == 'WIN'])\n",
    "losses = 5 - wins\n",
    "print(f\"\\n\\nPredicted Record: {wins}-{losses}\")\n",
    "print(f\"Win Rate: {wins/5*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, rf_pred)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Random Forest Confusion Matrix', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "\n",
    "# Gradient Boosting Confusion Matrix\n",
    "cm_gb = confusion_matrix(y_test, gb_pred)\n",
    "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Greens', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Gradient Boosting Confusion Matrix', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Actual')\n",
    "axes[0, 1].set_xlabel('Predicted')\n",
    "\n",
    "# Bagging Confusion Matrix\n",
    "cm_bag = confusion_matrix(y_test, bagging_pred)\n",
    "sns.heatmap(cm_bag, annot=True, fmt='d', cmap='Reds', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Bagging Confusion Matrix', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "\n",
    "# Voting Classifier Confusion Matrix\n",
    "cm_voting = confusion_matrix(y_test, voting_pred)\n",
    "sns.heatmap(cm_voting, annot=True, fmt='d', cmap='Oranges', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Voting Classifier Confusion Matrix', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Actual')\n",
    "axes[1, 1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Prediction visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if p == 'WIN' else 'red' for p in predictions_df['Prediction']]\n",
    "bars = ax.bar(predictions_df['Opponent'], predictions_df['Win Probability'].str.rstrip('%').astype(float), color=colors, alpha=0.7)\n",
    "ax.set_title('OKC Thunder Next 5 Games Predictions', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Win Probability (%)', fontsize=12)\n",
    "ax.set_xlabel('Opponent', fontsize=12)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.axhline(y=50, color='black', linestyle='--', alpha=0.3, label='50% threshold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, prob) in enumerate(zip(bars, predictions_df['Win Probability'].str.rstrip('%').astype(float))):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{prob:.1f}%',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('okc_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
